{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pyspark_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1qg_6rGeWzV3BAD3AKPlV4tulLvB0-yjx",
      "authorship_tag": "ABX9TyPTGVt6rHu7Bsa2HEFybKeb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foukonana/Diabetes/blob/master/Pyspark_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdYv1OtWrRoi"
      },
      "source": [
        "## Introduction\n",
        "\\\n",
        "PySpark is an interface of Apache Spark in Python. It allows one to write Spark application using the Python APIs.\n",
        "\\\n",
        "Some of the super advantages of PySpark are:\n",
        "* PySpark is a general-purpose, in-memory, distributed processing engine that allows you to process data efficiently in a **distributed** fashion.\n",
        "* Applications running on PySpark are **100x faster** than traditional systems.\n",
        "* Using PySpark we can process data from Hadoop HDFS, AWS S3, and many file systems.\n",
        "\n",
        "### Architecture  \n",
        "Apache Spark works in a master-slave architecture where the master is called “Driver” and slaves are called “Workers”. When you run a Spark application, Spark Driver creates a context that is an entry point to your application, and all operations (transformations and actions) are executed on worker nodes, and the resources are managed by Cluster Manager.\n",
        "\\\n",
        "![architecture](https://i2.wp.com/sparkbyexamples.com/wp-content/uploads/2020/02/spark-cluster-overview.png?w=596&ssl=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOcYTSg3QGrH"
      },
      "source": [
        "### The basics  \n",
        "Every Spark application consists of a _driver program_. This, runs the main function and executes various parallel operations on a cluster.   \n",
        "We initialize Spark with a **SparkContext** object. SparkContext contains all the accesing to cluster information. Before creating a SparkContext, **SparkConf** object needs to be build - it contains the information about the application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "panJMYA8QsAQ",
        "outputId": "076f1b82-a5de-4bae-9790-866334273fd0"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 63kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 17.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=27ef751825a1538240a64fd59b7508158f8c1bc7fe47929f83ca5a50273948e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzHUDvrdQsCl"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8VjZGL7QsFL"
      },
      "source": [
        "conf = SparkConf()\\\n",
        "        .setAppName('tutorial')\\\n",
        "        .setMaster('local')\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL8Y-hAOrUH_"
      },
      "source": [
        "### PySpark RDDs [Resilient Distributed Datasets]  \n",
        "An RDD is a fault-tolerant, immuatable collection of elements that can be operated on in parallel. Each RDD is divided into logical partitions, which can be computed on different cluster nodes.  \n",
        "\\\n",
        "Parallelized collections are created by calling SparkContext _parallelize_ methods on an existing iterable or collections. The elements of the iterable are comptied and form a distributed dataset that can be operated on in parallel. **Partitions** of the dataset can also be defined inside paralelize, otherwise they are set by default to the number of partitions of the cluster. (typically, 2-4 partitions for each CPU in the cluster).  \n",
        "\\\n",
        "_Spark's power to fast processing of large quantities of data comes from the partitiones. Spark tasks are created and performed on each partition. Thus, each job is operated on a smalled dataset._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVPo62HPTkAz"
      },
      "source": [
        "data = list(range(100))\n",
        "\n",
        "data_rdd = sc.parallelize(data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3RYkB1qTkDW",
        "outputId": "a78103d2-e970-40ed-f1ba-664da792daa2"
      },
      "source": [
        "type(data_rdd)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he3mdVUZkEz-"
      },
      "source": [
        "PySpark distributed datasets van be created from any Hadoop supported storage (S3, local file system, HDFS etc).  \n",
        "Text file RDDs are created using **SparkContext.textFile**.\n",
        "A URI of the local file path or a s3:// can be used.  \n",
        "The data on the file are not loaded in memory. data_rdd is just a pointer to the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_dQw5XPtmst"
      },
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "# read the data into 4 partitions\n",
        "data_rdd = sc.textFile(file_path + 'water_potability.csv', 4)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWmQSWjtJMLh",
        "outputId": "8538d9d3-b31f-48bd-f831-c8e52fe5ee2a"
      },
      "source": [
        "data_rdd.take(5)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ph,Hardness,Solids,Chloramines,Sulfate,Conductivity,Organic_carbon,Trihalomethanes,Turbidity,Potability',\n",
              " ',204.8904554713363,20791.318980747026,7.300211873184757,368.51644134980336,564.3086541722439,10.3797830780847,86.9909704615088,2.9631353806316407,0',\n",
              " '3.71608007538699,129.42292051494425,18630.057857970347,6.635245883862,,592.8853591348523,15.180013116357259,56.32907628451764,4.500656274942408,0',\n",
              " '8.099124189298397,224.23625939355776,19909.541732292393,9.275883602694089,,418.6062130644815,16.868636929550973,66.42009251176368,3.0559337496641685,0',\n",
              " '8.316765884214679,214.37339408562252,22018.417440775294,8.05933237743854,356.88613564305666,363.2665161642437,18.436524495493302,100.34167436508008,4.628770536837084,0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-VbDCTcoIPv"
      },
      "source": [
        "### RDD Operations  \n",
        "There are two types of operations: \n",
        "* **transformations** --> create new dataset from existing one\n",
        "* **actions** --> return a values after running the computation on the dataset  \n",
        "\n",
        "\n",
        "_Spark transformations are lazy. They are computed only when an action requires a result to be returned._  \n",
        "\n",
        "By default, each transformed RDD is recumputed everytime we run and action on it. We can use the **persist** method to save the results of RDD evaluation in cache memory for reduction in computations, if we need to access these data for further computations.    \n",
        "Find examples [here](https://blog.knoldus.com/understanding-persistence-in-apache-spark/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehOSzVh4_cxw"
      },
      "source": [
        "#### Basic/Most common PySpark transformations and actions\n",
        "Tranformation category | Transformation      |  Explanation\n",
        "-----------------------|---------------------|------------------\n",
        "General | map(func)           | Each element on the RDD where .map is applied, is passed through a function func. Input and output RDD will have the same number of elements.\n",
        "General | flatMap(func)       | Produces multiple output elements for each input element. _Applies the function to all elements of the RDD and then flattens the results_\n",
        "General | filter()        | Return RDD with only the elements that satisfy the condition(s).\n",
        "General | groupByKey(), reduceByKey() | These transformation are applied on a (key, value) pair RDD. Values are grouped by each key in the original RDD. \n",
        "Math/ Statistical | sample(withReplacement, fraction) | Sample a fraction _fraction_ of the data with or withour replacement\n",
        "Set theory/ Relational | union(otherDataset) | Return a new RDD that contains the union of the original RDD and the argument\n",
        "Set theory/ Relational | distinct()          | Return a new RDD that contains the distinct elements contained in the source dataset\n",
        "Data Structure/ I/O | coalelse() | ??\n",
        "\n",
        "\\\n",
        "\n",
        "Action category | Action      |  Explanation\n",
        "-----------------------|---------------------|------------------\n",
        "General  | getNumPartitions() |  Returns the number of partions of the RDD\n",
        "General | reduce() | Aggregate the elements of the dataset using a function func (which takes two arguments and returns one)\n",
        "General | collect() | Return all the elements of the dataset as an array at the driver \n",
        "General | take(n) |  Take and display n sample elements from an RDD\n",
        "General | first(), last() | Take the first (last) element from an RDD and display it\n",
        "Math/ Statistical | count() | Count the number of elements in the RDD\n",
        "Math/ Statistical | min(), max(), sum(), avg(), stdev() | Classic statistic measures\n",
        "\n",
        "More on flatMap here: [map vs flatMap](https://data-flair.training/blogs/apache-spark-map-vs-flatmap/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M8JVUm6hQ7Q",
        "outputId": "645c1394-ebd4-4c22-9b1c-4d22c63ab037"
      },
      "source": [
        "data_rdd.getNumPartitions()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfxqAX0c5tQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78f9429-0b1a-43fd-ae72-824c3b5beced"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "# map the data to seperate the values of the rows \n",
        "data_rdd = data_rdd.map(lambda s: s.split(sep=','))\n",
        "\n",
        "# use filter method to keep the header row and the data seperately \n",
        "header = data_rdd.first()\n",
        "data_rdd = data_rdd.filter(lambda row: row!=header)\n",
        "\n",
        "# all values are float, if missing replace with NaN\n",
        "data_rdd = data_rdd.map(lambda row: [np.float(x) if x!='' else np.nan for x in row])\n",
        "\n",
        "# persit so that we can access faster without re_calculating all previous steps\n",
        "data_rdd.persist()\n",
        "\n",
        "# all previous steps could be in the same pipeline"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[146] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkZ8PMQEK6lf",
        "outputId": "19d66339-fdb8-4f99-ea33-a1825d70fa41"
      },
      "source": [
        "# potability is a feature that shows if the water can be drinked\n",
        "# it should be binary (values 0 and 1) --> find the values with distinct\n",
        "potability = data_rdd.map(lambda row: row[-1]).distinct()\n",
        "# use collect method(action) to have the values returned\n",
        "potability.collect()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEYAtDH7VbW0",
        "outputId": "2c55a853-ea36-492a-fb4f-a896de9e24cd"
      },
      "source": [
        "# find statistics of water pH(1st column) and chloramines(4th column) per potability value\n",
        "grouped_rdd = data_rdd.map(lambda row: (row[-1], [row[0], row[3]])).groupByKey()\n",
        "grouped_rdd.collect()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.0, <pyspark.resultiterable.ResultIterable at 0x7ff3e8345b90>),\n",
              " (1.0, <pyspark.resultiterable.ResultIterable at 0x7ff3e819a590>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP9HVqXyzRHC"
      },
      "source": [
        "For someone that feels comfortable with incorporating python Pandas functionality in his pipelines, switching to an RDD-heavy logic can be quite frustrating.  \n",
        "Instead of working with RDDs, PySpark has a DataFrame API.  \n",
        "An RDD can be converted to a PySpark dataframe with .toDF() method.  \n",
        "\n",
        "\\\n",
        "Although the actions that a user can do on a Python Pandas and a PySpark DataFrame are very similar, there are a few differences:\n",
        "\n",
        "Pandas DataFrame | PySpark DataFrame\n",
        "-----------------|--------------------\n",
        " - [x]   | Operation run on parallel on different nodes in the cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBnJdlE1wfmv",
        "outputId": "b8af4476-3c08-4a59-aaaa-c1882bff6b5f"
      },
      "source": [
        "df = data_rdd.toDF(header)\n",
        "type(df)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzehvQzu5Ttf",
        "outputId": "16cca9b9-31f2-4f4b-a70f-f4300e9022eb"
      },
      "source": [
        "# datatypes of columns \n",
        "df.printSchema()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- ph: double (nullable = true)\n",
            " |-- Hardness: double (nullable = true)\n",
            " |-- Solids: double (nullable = true)\n",
            " |-- Chloramines: double (nullable = true)\n",
            " |-- Sulfate: double (nullable = true)\n",
            " |-- Conductivity: double (nullable = true)\n",
            " |-- Organic_carbon: double (nullable = true)\n",
            " |-- Trihalomethanes: double (nullable = true)\n",
            " |-- Turbidity: double (nullable = true)\n",
            " |-- Potability: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJRiB_7E5TwO",
        "outputId": "3eff2c59-491c-4a26-a245-09d6c926b98f"
      },
      "source": [
        "# the dataset has still the partitions that we defined when we created the pointer with the RDD\n",
        "df.rdd.getNumPartitions()"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYlPNb-g5Tyz",
        "outputId": "b6d2b496-6163-458b-dab4-52b4b8d72a08"
      },
      "source": [
        "# show values of selected columns\n",
        "df.select('ph', 'Sulfate', 'Potability').show(5)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+------------------+----------+\n",
            "|               ph|           Sulfate|Potability|\n",
            "+-----------------+------------------+----------+\n",
            "|              NaN|368.51644134980336|       0.0|\n",
            "| 3.71608007538699|               NaN|       0.0|\n",
            "|8.099124189298397|               NaN|       0.0|\n",
            "|8.316765884214679|356.88613564305666|       0.0|\n",
            "|9.092223456290965|310.13573752420444|       0.0|\n",
            "+-----------------+------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFPoUeGC5T1U",
        "outputId": "55169d6f-1581-41b8-e21d-e213832b3e0f"
      },
      "source": [
        "# just like pandas, column-wise summary statistics can be accesed with describe or summary methods\n",
        "df.summary().show()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
            "|summary|               ph|          Hardness|            Solids|       Chloramines|           Sulfate|      Conductivity|    Organic_carbon|   Trihalomethanes|         Turbidity|         Potability|\n",
            "+-------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
            "|  count|             3276|              3276|              3276|              3276|              3276|              3276|              3276|              3276|              3276|               3276|\n",
            "|   mean|              NaN|196.36949601730146|22014.092526077104| 7.122276793425785|               NaN| 426.2051106825533|14.284970247677315|               NaN| 3.966786169791058| 0.3901098901098901|\n",
            "| stddev|              NaN|32.879761476294156| 8768.570827785928|1.5830848890397096|               NaN| 80.82406405111182|3.3081619991268725|               NaN|0.7803824084854124|0.48784916967025493|\n",
            "|    min|              0.0|            47.432|  320.942611274359|0.3520000000000003|129.00000000000003|  181.483753985146|2.1999999999999886|0.7379999999999995|              1.45|                0.0|\n",
            "|    25%|6.277463093365682|176.84106250266586|15661.940335043397| 6.126270002317292| 317.0668424405928| 365.7202915004255|12.065316925324225|  56.6358113166401|3.4396233538018897|                0.0|\n",
            "|    50%|7.384048887654537|196.95287516726086|20922.154463374245| 7.130161148170633| 348.9000156544668|421.87985373320026|14.217372572687509| 67.62261619228332|3.9549642917273493|                0.0|\n",
            "|    75%| 8.92398060773871|216.66531857869694|27331.361961927756|  8.11473101538499| 426.1575185722385|   481.77193425228|16.557176634632867| 79.16959735728713| 4.500207624732678|                1.0|\n",
            "|    max|              NaN|           323.124| 61227.19600771213|13.127000000000002|               NaN| 753.3426195583046| 28.30000000000001|               NaN|             6.739|                1.0|\n",
            "+-------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4TW8Ft0BYJV",
        "outputId": "44a140b4-c125-42cb-b74f-c11d4ba573b9"
      },
      "source": [
        "# check for null values \n",
        "from pyspark.sql.functions import when, count, isnan\n",
        "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+------+-----------+-------+------------+--------------+---------------+---------+----------+\n",
            "| ph|Hardness|Solids|Chloramines|Sulfate|Conductivity|Organic_carbon|Trihalomethanes|Turbidity|Potability|\n",
            "+---+--------+------+-----------+-------+------------+--------------+---------------+---------+----------+\n",
            "|491|       0|     0|          0|    781|           0|             0|            162|        0|         0|\n",
            "+---+--------+------+-----------+-------+------------+--------------+---------------+---------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVI2c4-IFUd0",
        "outputId": "a8955ea2-c291-4054-893f-3f2a0e8af1e5"
      },
      "source": [
        "nrows = df.count()\n",
        "df.select([(count(when(isnan(c), c))/nrows).alias(c)for c in df.columns]).show()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+--------+------+-----------+-------------------+------------+--------------+-------------------+---------+----------+\n",
            "|                 ph|Hardness|Solids|Chloramines|            Sulfate|Conductivity|Organic_carbon|    Trihalomethanes|Turbidity|Potability|\n",
            "+-------------------+--------+------+-----------+-------------------+------------+--------------+-------------------+---------+----------+\n",
            "|0.14987789987789987|     0.0|   0.0|        0.0|0.23840048840048841|         0.0|           0.0|0.04945054945054945|      0.0|       0.0|\n",
            "+-------------------+--------+------+-----------+-------------------+------------+--------------+-------------------+---------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVKXgiZ6GIZT",
        "outputId": "8b7d542e-4f11-427c-c7a4-5d26d590a803"
      },
      "source": [
        "from pyspark.sql.functions import mean\n",
        "df.select('ph').na.drop().select(mean('ph')).show()"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|           avg(ph)|\n",
            "+------------------+\n",
            "|7.0807945042768345|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv7JoeJjK7FU",
        "outputId": "f9d13551-97b0-43dc-ad07-4b9542cce292"
      },
      "source": [
        "df.groupby('potability').agg({'Chloramines': 'mean'}).show()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----------------+\n",
            "|potability| avg(Chloramines)|\n",
            "+----------+-----------------+\n",
            "|       0.0|7.092174563443739|\n",
            "|       1.0|7.169338026214628|\n",
            "+----------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoErQp5HLaw5",
        "outputId": "0fc77805-2d38-483a-a651-1ca77e0ad44f"
      },
      "source": [
        "df.groupby('potability').count().show()"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+\n",
            "|potability|count|\n",
            "+----------+-----+\n",
            "|       0.0| 1998|\n",
            "|       1.0| 1278|\n",
            "+----------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B13ZS68LoMT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}